在[[多项式拟合]]中，假设我们只需要控制一个参数，而其他的参数都已经到了最佳位置，我们就能获得该参数 k 在这个多项式的损失函数的数个点 (k, y) 及其导数。

梯度下降就是利用这两个值，根据其导数判断 k 应该向哪里调整 y 更小，如何向那里移动一些之后再重复

<font color="#ff0000">可是有多个极小值怎么办？</font> #疑问  随机选定 k 值然后分别梯度下降既不可靠又性能低下。
UP 说：**如果损失曲面坑坑洼洼，梯度优化陷入局部极小值怎么办？非常好的问题，事实上，这个问题也一直困扰着专业研究者，直到现在都是一个open problem。实践中找到的局部极小值，似乎和全局最小值没什么本质差别，也有很好的泛化和测试性能，深刻的原因不得而知。**
	有人答：**一种解释是这样的，实际上高维空间中很多梯度为0的点并不是极值，而是鞍点，在训练中往往可以通过批次轮换跳出鞍点。当然这只是一种猜测，高维空间的表面长什么样我们都没见过。**

---
**那如果在实际情况下，所有的 k 都没有调好怎么办？**
1. 我们先讨论有两个参数未知的情况。这种情况下得到结果是梯度下降，在 n 维度、的损失函数中都可以用。
	1. 这种情况，损失函数是一个二元函数。因为有两个自变量和一个因变量，可以表示在一个 xyz 三维空间中，x、y 轴是两个 k 参数，z 轴是 y。这种情况下，损失函数是一个损失曲面。 
	2. 但是这种情况下可以调整两个参数，对应的导数是什么呢？这里需要使用[[偏导数]]，**偏导数合在一起就是[[梯度]]**，就是 N 维图像某个给定点函数变化最快的方向（最陡峭的方向）。
	3. 沿着梯度下降就能找到这个损失函数最低点。这就是「梯度下降」。
2. 如何找到梯度？使用[[反向传播]]。
	1. 为什么这个算法能行？因为我们知道任意函数例如 k，到最终损失函数都是经过层层函数的嵌套，我们又可以利用函数和函数的导数求出嵌套函数，所以可以反推整个过程。**于是，我们从 k 推导出中间所有函数及其导数和损失函数及其导数，又倒推回 k，就能得到 k 相对于输出损失函数的梯度**。 #待完善 
	2. 有了梯度再用梯度下降即可。