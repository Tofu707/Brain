上接[[一口气讲完人工智能 10.21]]

---
#### 开篇
1. 
#### 提示
1. 
#### 资源
- 
#### 图表

### 笔记
#### 讲义、学习内容
1. 语言大模型理解语言的方式
	1. 人类如何理解语言？
		1. 我们的句子遵照一定的语法和词义。
		2. 尽管我们不会死板用语法，但是我们说出口之前对要说的东西已经有一个大概的感觉和明白结构了。
	2. 大模型理解语言
		1. **接话尾**：说到一半压根不知道后面的东西是啥，输出到一半要现时推理接下来应该说什么，就像疯狂做完形填空。
2. 为什么理解方式不同，但是却不违和？**机器如何搞明白语言的规律？**
	1. 什么是语言的规律？
		1. 语言的功能最终还是沟通，沟通的基本符号单位是[[语素]]，机器的语素就是 Token。
		2. 语言规律就是理解每一个[[语素]]的含义，并且明白他们要以什么样的顺序出现，这个序列传递了什么意思。
	2. [[计算语言学]]：机器通过计算下一个 token 要用啥来理解语言。这个用来算下一个 token 是什么的函数就是语言模型。
	3. 机器理解语言的方法
		1. 使用语法[[语法树]]，[[隐马尔科夫模型 HMM]]，[[长短记忆力 LSTM]]，[[循环神经网络 RNN]]。
		2. 使用先前语句尾部来推测接话尾，有点像用输入法一直点联想词[[统计语言模型]]。
			1. 输入法的接话尾：1-Gram Model（或 n-Gram, 几个字）。仅仅使用前一个字的最常见搭配来组句。
3. [[生成式预训练变换器 GPT]]
	1. 和 n-Gram Model 不一样，GPT 用来预测的是先前的所有内容（或者预设的长度）。
	2. [[LLM 预训练]]的方法之一是 预测下一个语素 next-token-prodiction。
		1. 为什么这种接龙可以获得这么强大的语言理解能力？因为前文提供的信息可以呈现出语言的准确规律。实际上，这种接话尾游戏可以极度深入地理解语言，不然完形填空就很好做了。
		2. 这种训练方法不需要人工标注，只需要数量大、质量高的文本（[[语料]]）即可。
	3. GPT 的这些特性带来的缺点：
		1. 语料需求量高。GPT 的语料保守估计是 TB 级别。
		2. 消耗算力高。GPT 3 说一个字要调整 1750 亿个参数。
		3. 部分特殊要求做不到，例如说一句恰好 20 字的话。
		4. 出现[[幻觉]]。
	4. GPT 的局限性：
		1. GPT 仅仅看见现实世界投射到语言上的影子，而误以为其是现实世界。无论 LLM 多理解从语言看到的世界，也终究不是现实世界。这就是说，它只能看见[[符号界]]。如同柏拉图的[[洞穴之寓]]。
#### 课后提问
- 
### 总结
1. 
